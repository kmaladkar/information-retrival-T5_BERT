{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "po2WG2squ7fn"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install sacrebleu\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxLL-zwKuxZ7"
      },
      "source": [
        "import os, json\n",
        "import torch\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from collections import defaultdict\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import gc\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "import pickle"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6alSF7tlu0Qs",
        "outputId": "246a47f3-3f66-461f-8127-76d70428a7b1"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fK55t5RCXGB3"
      },
      "source": [
        "path_to_data = '/content/drive/MyDrive/Colab Notebooks/data/'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz6dxHsEypAk"
      },
      "source": [
        "class TrainerDataset(Dataset):\n",
        "    def __init__(self, path):\n",
        "        df = pd.read_json(path, lines=True)\n",
        "        df = df.dropna()\n",
        "        self.dataset = df\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        source = self.dataset.iloc[idx, 0]\n",
        "        target = self.dataset.iloc[idx, 1]\n",
        "        input_ids = self.tokenizer.encode(source, \n",
        "                                          return_tensors='pt',\n",
        "                                          padding='max_length', \n",
        "                                          truncation='longest_first', \n",
        "                                          max_length=512)[0]\n",
        "        \n",
        "        label = self.tokenizer.encode(target, \n",
        "                                      return_tensors='pt', \n",
        "                                      padding='max_length',\n",
        "                                      truncation='longest_first', \n",
        "                                      max_length=2)[0]\n",
        "        \n",
        "        return {'input_ids': input_ids, 'labels': label}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JNWw9sFlRec"
      },
      "source": [
        "def compute_T5_metrics(qids_to_relevant_passageids, qids_to_ranked_candidate_passages, MaxMRRRank):\n",
        "    \"\"\"Compute MRR metric\n",
        "    Args:   \n",
        "\n",
        "    p_qids_to_relevant_passageids (dict): dictionary of query-passage mapping\n",
        "    \n",
        "    p_qids_to_ranked_candidate_passages (dict): dictionary of query-passage candidates\n",
        "\n",
        "    Returns:\n",
        "        dict: dictionary of metrics {'MRR': <MRR Score>}\n",
        "    \"\"\"\n",
        "    all_scores = {}\n",
        "    MRR = 0\n",
        "    qids_with_relevant_passages = 0\n",
        "    ranking = []\n",
        "    \n",
        "    for qid in qids_to_ranked_candidate_passages:\n",
        "    \n",
        "        if qid in qids_to_relevant_passageids:\n",
        "            ranking.append(0)\n",
        "            target_pid = qids_to_relevant_passageids[qid]\n",
        "            candidate_pid = qids_to_ranked_candidate_passages[qid]\n",
        "            if len(candidate_pid) > 0 :\n",
        "              if len(candidate_pid) < MaxMRRRank:\n",
        "                MaxMRRRank = len(candidate_pid)\n",
        "              for i in range(0,MaxMRRRank):\n",
        "                if candidate_pid[i] in target_pid:\n",
        "                    MRR += 1/(i + 1)\n",
        "                    ranking.pop()\n",
        "                    ranking.append(i+1)\n",
        "                    break\n",
        "\n",
        "    print(MRR)\n",
        "    if len(ranking) == 0:\n",
        "        raise IOError(\"No matching QIDs found. Are you sure you are scoring the evaluation set?\")\n",
        "    MRR = MRR/len(qids_to_relevant_passageids)\n",
        "    all_scores['MRR @10'] = MRR\n",
        "    all_scores['QueriesRanked'] = len(qids_to_ranked_candidate_passages)\n",
        "    return all_scores"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkIxQwxijqJe"
      },
      "source": [
        "def convert_to_evaluation(lol_of_dists, top1000_dict):\n",
        "  \"\"\"Converts the output of the model into the required format for MRR ranking\"\"\"\n",
        "  eval_dict = defaultdict(list)\n",
        "\n",
        "  lol_idx = 0\n",
        "  for Q_ID, value in top1000_dict.items():\n",
        "    \n",
        "    probs = []\n",
        "    while len(probs) < len(value):\n",
        "      probs += lol_of_dists[lol_idx]\n",
        "      lol_idx += 1\n",
        "\n",
        "    assert len(probs) == len(value), f\"{len(probs)}, {len(value)}, {Q_ID}\"\n",
        "    for idx, ele in enumerate(probs):\n",
        "      eval_dict[Q_ID].append((value[idx], ele))\n",
        "\n",
        "    eval_dict[Q_ID] = sorted(eval_dict[Q_ID], key=lambda x: x[1][0], reverse=True)\n",
        "    eval_dict[Q_ID] =[x[0] for x in eval_dict[Q_ID] if x[1][0] > .50]\n",
        "\n",
        "  return eval_dict\n",
        "  "
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xp32ffaUBWou"
      },
      "source": [
        "def convert_to_jsonl(file_path, relevance_dict, split='train'):\n",
        "  t5_file = open(file_path, 'w')\n",
        "  if split != 'train':\n",
        "      for key, value in relevance_dict.items():\n",
        "        q_id = key\n",
        "        for p_id in value[:]: \n",
        "          t5_out = {}\n",
        "          t5_out['input_text'] = \"Query: \" + QUERY_DICT[q_id] + \" Document: \" + PASSAGE_DICT[p_id] + \" Relevance: \"\n",
        "          t5_out['target_text'] = ''\n",
        "          t5_file.write(json.dumps(t5_out))\n",
        "          t5_file.write('\\n')\n",
        "      t5_file.close()\n",
        "  else:\n",
        "      for key, value in relevance_dict.items():\n",
        "        q_id, pp_id = key\n",
        "        t5_out = {}\n",
        "        t5_out['input_text'] = \"Query: \" + QUERY_DICT[q_id] + \" Document: \" + PASSAGE_DICT[pp_id] + \" Relevance: \"\n",
        "        #t5_out['Document'] = PASSAGE_DICT[pp_id]\n",
        "        t5_out['target_text'] = 'true'\n",
        "        t5_file.write(json.dumps(t5_out))\n",
        "        t5_file.write('\\n')\n",
        "        for np_id in value[:10]: \n",
        "          t5_out = {}\n",
        "          t5_out['input_text'] = \"Query: \" + QUERY_DICT[q_id] + \" Document: \" + PASSAGE_DICT[np_id] + \" Relevance: \"\n",
        "          t5_out['target_text'] = 'false'\n",
        "          t5_file.write(json.dumps(t5_out))\n",
        "          t5_file.write('\\n')\n",
        "      t5_file.close()\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYj11zrNdQ-0"
      },
      "source": [
        "def load_triplets(path, split, trip_dict):\n",
        "  with open(path_to_data + f'diverse.triplets.{split}.tsv', 'r', encoding='utf-8') as inF:\n",
        "    for line in inF:\n",
        "      q_id, pp_id, np_id = line.split('\\t')\n",
        "      trip_dict[(q_id, pp_id)].append(np_id.strip('\\n'))\n",
        "  inF.close()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k07NoOYWCmwT"
      },
      "source": [
        "def enforce_correct_size(rel_dict, split='train'):\n",
        "  \"\"\"Enforces each Query has atleast 10 documents\"\"\"\n",
        "  removed_keys = []\n",
        "\n",
        "  for key, value in rel_dict.items():\n",
        "\n",
        "    if split == 'train':\n",
        "      if len(value) < 10:\n",
        "        removed_keys.append(key)\n",
        "    else:\n",
        "      if len(value) < 1000:\n",
        "        removed_keys.append(key)\n",
        "  \n",
        "  for key in removed_keys:\n",
        "    del rel_dict[key]\n",
        "  return rel_dict"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8_3LGdCVXPk"
      },
      "source": [
        "def remove_duplicate_queries(rel_dict):\n",
        "  \"\"\"Removes duplicate queries\"\"\"\n",
        "  queries = []\n",
        "  for key, value in rel_dict.items():\n",
        "    queries.append(key[0])\n",
        "\n",
        "  duplicates = [x for x in queries if queries.count(x) >= 2]\n",
        "  removed_keys = []\n",
        "  for key, value in rel_dict.items():\n",
        "    if key[0] in duplicates:\n",
        "      removed_keys.append(key)\n",
        "\n",
        "  for key in removed_keys:\n",
        "    del rel_dict[key]\n",
        "  return rel_dict\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eva7IAuYXQiA"
      },
      "source": [
        "PASSAGE_DICT = defaultdict(str)\n",
        "QUERY_DICT = defaultdict(str)\n",
        "\n",
        "with open(path_to_data + 'diverse.passages.all.tsv', 'r', encoding='utf-8') as inF:\n",
        "  for line in inF:\n",
        "    p_id, passage = line.split('\\t')\n",
        "    PASSAGE_DICT[p_id] = passage[:-1]\n",
        "\n",
        "with open(path_to_data + 'diverse.queries.all.tsv', 'r', encoding='utf-8') as inF:\n",
        "  for line in inF:\n",
        "    q_id, query = line.split('\\t')\n",
        "    QUERY_DICT[q_id] = query[:-1]\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBtDqjjVkYxR"
      },
      "source": [
        "with open(path_to_data+'/top1000dict.pickle', \"rb\") as inF:\n",
        "    top_1000_dict = pickle.load(inF)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkMg9KuTCdzx"
      },
      "source": [
        "all_relevance_dict = defaultdict(list)\n",
        "train_relevance_dict = defaultdict(list)\n",
        "dev_relevance_dict = defaultdict(list)\n",
        "test_relevance_dict = defaultdict(list)\n",
        "\n",
        "load_triplets(path_to_data, 'all', all_relevance_dict)\n",
        "load_triplets(path_to_data, 'train', train_relevance_dict)\n",
        "load_triplets(path_to_data, 'dev', dev_relevance_dict)\n",
        "load_triplets(path_to_data, 'test', test_relevance_dict)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OQATLVGCk7v"
      },
      "source": [
        "train_relevance_dict = enforce_correct_size(train_relevance_dict)\n",
        "dev_relevance_dict = enforce_correct_size(dev_relevance_dict)\n",
        "test_relevance_dict = enforce_correct_size(test_relevance_dict)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsyP-wxJWiB1"
      },
      "source": [
        "dev_relevance_dict = remove_duplicate_queries(dev_relevance_dict)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAfg7IM-lPun"
      },
      "source": [
        "TEST_QUERIES = [x[0] for x in list(test_relevance_dict.keys())]\n",
        "DEV_QUERIES = [x[0] for x in list(dev_relevance_dict.keys())]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf8a_QeelzhX"
      },
      "source": [
        "top_1000_test = defaultdict(list)\n",
        "for Q in TEST_QUERIES:\n",
        "  top_1000_test[Q] = top_1000_dict[Q]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKyVlbMUn4Z7"
      },
      "source": [
        "top_1000_dev = defaultdict(list)\n",
        "for Q in DEV_QUERIES:\n",
        "  top_1000_dev[Q] = top_1000_dict[Q]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KTRYcafMsjO"
      },
      "source": [
        "top_1000_test2 = enforce_correct_size(top_1000_test, split='test')\n",
        "top_1000_dev2 = enforce_correct_size(top_1000_dev, split='dev')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlAsb37KsVTI"
      },
      "source": [
        "qrels_dict = defaultdict(list)\n",
        "with open(path_to_data+'/qrels.train.tsv', 'r', encoding='utf-8') as inF:\n",
        "  for line in inF:\n",
        "    Q_ID, _, P_ID, _ = line.split('\\t')\n",
        "    qrels_dict[Q_ID].append(P_ID)\n",
        "\n",
        "test_qrels = defaultdict(list)\n",
        "for Q in top_1000_test2.keys():\n",
        "  test_qrels[Q] = qrels_dict[Q]"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHsSJ53zTewt",
        "outputId": "a75d68f7-0f85-4e04-fcc0-0d5c32fa7dd5"
      },
      "source": [
        "print(len(top_1000_test2))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqCO1vKwDmXE",
        "outputId": "f9344897-8aa0-4451-b7c9-a1b16eb7aef5"
      },
      "source": [
        "### Make sure all our Queries have atleast 10 passages\n",
        "for key, value in train_relevance_dict.items():\n",
        "  assert len(value) > 9\n",
        "\n",
        "for key, value in dev_relevance_dict.items():\n",
        "  assert len(value) > 9\n",
        "\n",
        "for key, value in test_relevance_dict.items():\n",
        "  assert len(value) > 9\n",
        "\n",
        "print(\"Success!\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZVNLw68Bl-v"
      },
      "source": [
        "convert_to_jsonl(path_to_data+'/json/train.json', train_relevance_dict)\n",
        "convert_to_jsonl(path_to_data+'/json/dev.json', top_1000_dev2, split='dev')\n",
        "convert_to_jsonl(path_to_data+'/json/test.json', top_1000_test2, split='test')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6gWLeaXvkEZ"
      },
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained('t5-base')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2G-e5Djy2Pqd"
      },
      "source": [
        "train_dataset = TrainerDataset(path_to_data+'/json/train.json')\n",
        "dev_dataset = TrainerDataset(path_to_data+'/json/dev.json')\n",
        "test_dataset = TrainerDataset(path_to_data+'/json/test.json')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLQp9Cb62m3M"
      },
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=11, shuffle=False)\n",
        "dev_dataloader = DataLoader(dev_dataset, batch_size=50, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=50, shuffle=False)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gh1gCUHvSaMG"
      },
      "source": [
        "TRUE_TOK_ID = 1176\n",
        "FALSE_TOK_ID = 6136"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfYrHHe03wd8",
        "outputId": "8d08628b-4427-45fe-930c-83fbc89e87b7"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fd7sUkYvYNKs",
        "outputId": "aab28019-aace-485e-a9e8-b7612d94ccdd"
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "204"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYYFhHfBX0Yh",
        "outputId": "f06900e8-2f1c-42f6-c01b-b1cb7099f5cf"
      },
      "source": [
        "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |       0    |       0    |       0    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |       0    |       0    |       0    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
            "|       from large pool |       0    |       0    |       0    |       0    |\n",
            "|       from small pool |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUJ5zXyIvuPq"
      },
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained('t5-base')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jio8Uw5J41Cy"
      },
      "source": [
        "model = model.to(device)\n",
        "weights = torch.ones(32128)\n",
        "weights[FALSE_TOK_ID] = .10\n",
        "loss_function = nn.CrossEntropyLoss(weight=weights.to(device))\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=.00003)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxEZBwg835Mf",
        "outputId": "2a62f545-3cce-4099-a45b-35927670b88e"
      },
      "source": [
        "print(f\"Number of Batches: {len(train_dataloader)}\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Batches: 3523\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1gUlHQz5We5"
      },
      "source": [
        "EPOCHS=1"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHPr6I924G4j"
      },
      "source": [
        "for epoch in range(1, EPOCHS+1):  \n",
        "\n",
        "    print(f\"Beginning Epoch {epoch}\")\n",
        "    for i, data in enumerate(train_dataloader):\n",
        "        \n",
        "        model.zero_grad()\n",
        "\n",
        "        input_ids, target_ids  = data['input_ids'], data['labels']\n",
        "\n",
        "        input_ids = input_ids.long().to(device)\n",
        "        target_ids = target_ids.long().to(device)\n",
        "\n",
        "        output = model(input_ids, labels=target_ids, return_dict=True)\n",
        "        loss = loss_function(output.logits.permute(0, 2, 1), target_ids)\n",
        "     \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f\"Batch {i}\")\n",
        "        print(f\"loss: {round(loss.item(), 4)}\")"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEeab-ahf9xw"
      },
      "source": [
        "model.save_pretrained('/content/ir-transformer/')"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmbk4Ih9py_d"
      },
      "source": [
        "model = model.from_pretrained('/content/drive/MyDrive/Colab Notebooks/ir-transformer-weighted')"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh7L6h-Mv_E1"
      },
      "source": [
        "model = model.to(device)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6Q3lhpiP_6b"
      },
      "source": [
        "# lol_of_dists = []\n",
        "\n",
        "# for i, data in enumerate(dev_dataloader):\n",
        "    \n",
        "#     model.eval()\n",
        "\n",
        "#     with torch.no_grad():\n",
        "        \n",
        "#         input_ids, target_ids  = data['input_ids'], data['labels']\n",
        "\n",
        "#         batch_labels = data['labels'][:, 0].cpu().detach().tolist()\n",
        "#         labels += batch_labels\n",
        "\n",
        "#         input_ids = input_ids.to(device)\n",
        "#         target_ids = target_ids.to(device)\n",
        "\n",
        "#         output = model(input_ids, labels=target_ids, return_dict=True)\n",
        "#         logits = output.logits.cpu().detach()\n",
        "\n",
        "#         batch_preds = torch.argmax(logits[:, 0], dim=1).tolist()\n",
        "#         predictions += batch_preds\n",
        "\n",
        "#         true_query = logits[:, 0, TRUE_TOK_ID]\n",
        "#         false_query = logits[:, 0, FALSE_TOK_ID]\n",
        "#         cat = torch.cat([true_query.unsqueeze(1), false_query.unsqueeze(1)], dim=1)\n",
        "#         soft = F.softmax(cat, dim=1).tolist()\n",
        "#         lol_of_dists.append(soft)\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Rz2aYoB3EOz"
      },
      "source": [
        "lol_of_dists = []\n",
        "\n",
        "for i, data in enumerate(test_dataloader):\n",
        "    print(i)\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        input_ids, target_ids  = data['input_ids'], data['labels']\n",
        "\n",
        "        input_ids = input_ids.to(device)\n",
        "        target_ids = target_ids.to(device)\n",
        "\n",
        "        output = model(input_ids, labels=target_ids, return_dict=True)\n",
        "\n",
        "        logits = output.logits\n",
        "\n",
        "        true_query = logits[:, 0, TRUE_TOK_ID]\n",
        "        false_query = logits[:, 0, FALSE_TOK_ID]\n",
        "        cat = torch.cat([true_query.unsqueeze(1), false_query.unsqueeze(1)], dim=1)\n",
        "        soft = F.softmax(cat, dim=1).tolist()\n",
        "        lol_of_dists.append(soft)\n",
        "        torch.cuda.empty_cache()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5tLdf4dSE1n"
      },
      "source": [
        "with open(path_to_data+'lol_of_dists.pickle', 'wb') as outF:\n",
        "  pickle.dump(lol_of_dists, outF)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8r0t5PCHKQ6"
      },
      "source": [
        "eval_dict = convert_to_evaluation(lol_of_dists, top_1000_test2)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qU0Gdf1vsy5E",
        "outputId": "ff8a66da-32e2-49f4-deec-2296b2e5a4dd"
      },
      "source": [
        "compute_T5_metrics(test_qrels, eval_dict, 10)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "327.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'MRR @10': 0.654, 'QueriesRanked': 500}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    }
  ]
}